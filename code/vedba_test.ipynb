{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ce4e956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Auto-install required packages if missing ----\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "required = ['pyreadr', 'numpy', 'pandas', 'tqdm', 'pyarrow']\n",
    "\n",
    "for package in required:\n",
    "    try:\n",
    "        __import__(package)\n",
    "    except ImportError:\n",
    "        print(f\"Package '{package}' not found. Installing...\")\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', package])\n",
    "        \n",
    "import pyreadr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from concurrent.futures import as_completed\n",
    "import ast\n",
    "\n",
    "import os\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# ---- Helper function: dy_acc ----\n",
    "def dy_acc(vect, win_size=7):\n",
    "    \"\"\"\n",
    "    Calculate the dynamic acceleration (dy_acc) for a given vector.\n",
    "    \"\"\"\n",
    "    if vect is None or len(vect) == 0:\n",
    "        raise ValueError(\"Input vector is empty or invalid.\")\n",
    "    \n",
    "    pad_size = int(win_size / 2 - 0.5)\n",
    "    padded = np.pad(vect, (pad_size, pad_size), constant_values=np.nan)\n",
    "    acc_vec = np.empty(len(vect))\n",
    "    acc_vec[:] = np.nan\n",
    "\n",
    "    for i in range(len(vect)):\n",
    "        window = padded[i : i + (2 * pad_size + 1)]\n",
    "        m_ave = np.nanmean(window)\n",
    "        acc_vec[i] = vect[i] - m_ave\n",
    "    \n",
    "    return acc_vec\n",
    "\n",
    "# ---- Vector Calculation Function ----\n",
    "def process_row(row):\n",
    "    \"\"\"\n",
    "    Process a single row to calculate dynamic acceleration components and derived metrics.\n",
    "    \"\"\"\n",
    "    x_component = np.abs(dy_acc(row['x_cal_array']))\n",
    "    y_component = np.abs(dy_acc(row['y_cal_array']))\n",
    "    z_component = np.abs(dy_acc(row['z_cal_array']))\n",
    "\n",
    "    vectorial_sum = np.sqrt(x_component**2 + y_component**2 + z_component**2)\n",
    "    ave_vedba_value = np.nanmean(vectorial_sum)\n",
    "\n",
    "    pitch = np.arctan2(x_component, np.sqrt(y_component**2 + z_component**2))\n",
    "    ave_pitch = np.nanmean(pitch)\n",
    "\n",
    "    return ave_vedba_value, ave_pitch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36bc2125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event.id</th>\n",
       "      <th>visible</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>data.decoding.software</th>\n",
       "      <th>eobs.acceleration.axes</th>\n",
       "      <th>eobs.acceleration.sampling.frequency.per.axis</th>\n",
       "      <th>eobs.accelerations.raw</th>\n",
       "      <th>eobs.key.bin.checksum</th>\n",
       "      <th>eobs.start.timestamp</th>\n",
       "      <th>import.marked.outlier</th>\n",
       "      <th>...</th>\n",
       "      <th>individual.taxon.canonical.name</th>\n",
       "      <th>tag.local.identifier</th>\n",
       "      <th>individual.local.identifier</th>\n",
       "      <th>study.name</th>\n",
       "      <th>local_timestamp</th>\n",
       "      <th>tag_local_identifier</th>\n",
       "      <th>group_id</th>\n",
       "      <th>x_cal</th>\n",
       "      <th>y_cal</th>\n",
       "      <th>z_cal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.244668e+10</td>\n",
       "      <td>true</td>\n",
       "      <td>2025-09-05 00:00:00+00:00</td>\n",
       "      <td>21</td>\n",
       "      <td>XYZ</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2319 2360 1744 2318 2360 1744 2318 2360 1744 2...</td>\n",
       "      <td>1.993922e+09</td>\n",
       "      <td>2025-09-05 00:00:00.000</td>\n",
       "      <td>false</td>\n",
       "      <td>...</td>\n",
       "      <td>Papio anubis</td>\n",
       "      <td>10362</td>\n",
       "      <td>24AB04_0V2Z</td>\n",
       "      <td>Baboons MBRP Mpala Kenya</td>\n",
       "      <td>2025-09-05 03:00:00+03:00</td>\n",
       "      <td>10362</td>\n",
       "      <td>Periwinkle</td>\n",
       "      <td>{'x1': 2319.0, 'x2': 2318.0, 'x3': 2318.0, 'x4...</td>\n",
       "      <td>{'y1': 2360.0, 'y2': 2360.0, 'y3': 2360.0, 'y4...</td>\n",
       "      <td>{'z1': 1744.0, 'z2': 1744.0, 'z3': 1744.0, 'z4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.244668e+10</td>\n",
       "      <td>true</td>\n",
       "      <td>2025-09-05 00:01:00+00:00</td>\n",
       "      <td>21</td>\n",
       "      <td>XYZ</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2530 2123 1899 2532 2124 1900 2532 2122 1898 2...</td>\n",
       "      <td>3.087902e+09</td>\n",
       "      <td>2025-09-05 00:01:00.000</td>\n",
       "      <td>false</td>\n",
       "      <td>...</td>\n",
       "      <td>Papio anubis</td>\n",
       "      <td>10362</td>\n",
       "      <td>24AB04_0V2Z</td>\n",
       "      <td>Baboons MBRP Mpala Kenya</td>\n",
       "      <td>2025-09-05 03:01:00+03:00</td>\n",
       "      <td>10362</td>\n",
       "      <td>Periwinkle</td>\n",
       "      <td>{'x1': 2530.0, 'x2': 2532.0, 'x3': 2532.0, 'x4...</td>\n",
       "      <td>{'y1': 2123.0, 'y2': 2124.0, 'y3': 2122.0, 'y4...</td>\n",
       "      <td>{'z1': 1899.0, 'z2': 1900.0, 'z3': 1898.0, 'z4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.244668e+10</td>\n",
       "      <td>true</td>\n",
       "      <td>2025-09-05 00:02:00+00:00</td>\n",
       "      <td>21</td>\n",
       "      <td>XYZ</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2524 2179 1897 2528 2162 1900 2529 2145 1903 2...</td>\n",
       "      <td>2.130966e+09</td>\n",
       "      <td>2025-09-05 00:02:00.000</td>\n",
       "      <td>false</td>\n",
       "      <td>...</td>\n",
       "      <td>Papio anubis</td>\n",
       "      <td>10362</td>\n",
       "      <td>24AB04_0V2Z</td>\n",
       "      <td>Baboons MBRP Mpala Kenya</td>\n",
       "      <td>2025-09-05 03:02:00+03:00</td>\n",
       "      <td>10362</td>\n",
       "      <td>Periwinkle</td>\n",
       "      <td>{'x1': 2524.0, 'x2': 2528.0, 'x3': 2529.0, 'x4...</td>\n",
       "      <td>{'y1': 2179.0, 'y2': 2162.0, 'y3': 2145.0, 'y4...</td>\n",
       "      <td>{'z1': 1897.0, 'z2': 1900.0, 'z3': 1903.0, 'z4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.244668e+10</td>\n",
       "      <td>true</td>\n",
       "      <td>2025-09-05 00:03:00+00:00</td>\n",
       "      <td>21</td>\n",
       "      <td>XYZ</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2298 2411 1787 2300 2411 1788 2299 2411 1785 2...</td>\n",
       "      <td>1.468880e+09</td>\n",
       "      <td>2025-09-05 00:03:00.000</td>\n",
       "      <td>false</td>\n",
       "      <td>...</td>\n",
       "      <td>Papio anubis</td>\n",
       "      <td>10362</td>\n",
       "      <td>24AB04_0V2Z</td>\n",
       "      <td>Baboons MBRP Mpala Kenya</td>\n",
       "      <td>2025-09-05 03:03:00+03:00</td>\n",
       "      <td>10362</td>\n",
       "      <td>Periwinkle</td>\n",
       "      <td>{'x1': 2298.0, 'x2': 2300.0, 'x3': 2299.0, 'x4...</td>\n",
       "      <td>{'y1': 2411.0, 'y2': 2411.0, 'y3': 2411.0, 'y4...</td>\n",
       "      <td>{'z1': 1787.0, 'z2': 1788.0, 'z3': 1785.0, 'z4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.244668e+10</td>\n",
       "      <td>true</td>\n",
       "      <td>2025-09-05 00:04:00+00:00</td>\n",
       "      <td>21</td>\n",
       "      <td>XYZ</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2286 2416 1782 2288 2415 1783 2288 2415 1782 2...</td>\n",
       "      <td>1.544969e+09</td>\n",
       "      <td>2025-09-05 00:04:00.000</td>\n",
       "      <td>false</td>\n",
       "      <td>...</td>\n",
       "      <td>Papio anubis</td>\n",
       "      <td>10362</td>\n",
       "      <td>24AB04_0V2Z</td>\n",
       "      <td>Baboons MBRP Mpala Kenya</td>\n",
       "      <td>2025-09-05 03:04:00+03:00</td>\n",
       "      <td>10362</td>\n",
       "      <td>Periwinkle</td>\n",
       "      <td>{'x1': 2286.0, 'x2': 2288.0, 'x3': 2288.0, 'x4...</td>\n",
       "      <td>{'y1': 2416.0, 'y2': 2415.0, 'y3': 2415.0, 'y4...</td>\n",
       "      <td>{'z1': 1782.0, 'z2': 1783.0, 'z3': 1782.0, 'z4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       event.id visible                 timestamp  data.decoding.software  \\\n",
       "0  4.244668e+10    true 2025-09-05 00:00:00+00:00                      21   \n",
       "1  4.244668e+10    true 2025-09-05 00:01:00+00:00                      21   \n",
       "2  4.244668e+10    true 2025-09-05 00:02:00+00:00                      21   \n",
       "3  4.244668e+10    true 2025-09-05 00:03:00+00:00                      21   \n",
       "4  4.244668e+10    true 2025-09-05 00:04:00+00:00                      21   \n",
       "\n",
       "  eobs.acceleration.axes  eobs.acceleration.sampling.frequency.per.axis  \\\n",
       "0                    XYZ                                           20.0   \n",
       "1                    XYZ                                           20.0   \n",
       "2                    XYZ                                           20.0   \n",
       "3                    XYZ                                           20.0   \n",
       "4                    XYZ                                           20.0   \n",
       "\n",
       "                              eobs.accelerations.raw  eobs.key.bin.checksum  \\\n",
       "0  2319 2360 1744 2318 2360 1744 2318 2360 1744 2...           1.993922e+09   \n",
       "1  2530 2123 1899 2532 2124 1900 2532 2122 1898 2...           3.087902e+09   \n",
       "2  2524 2179 1897 2528 2162 1900 2529 2145 1903 2...           2.130966e+09   \n",
       "3  2298 2411 1787 2300 2411 1788 2299 2411 1785 2...           1.468880e+09   \n",
       "4  2286 2416 1782 2288 2415 1783 2288 2415 1782 2...           1.544969e+09   \n",
       "\n",
       "      eobs.start.timestamp import.marked.outlier  ...  \\\n",
       "0  2025-09-05 00:00:00.000                 false  ...   \n",
       "1  2025-09-05 00:01:00.000                 false  ...   \n",
       "2  2025-09-05 00:02:00.000                 false  ...   \n",
       "3  2025-09-05 00:03:00.000                 false  ...   \n",
       "4  2025-09-05 00:04:00.000                 false  ...   \n",
       "\n",
       "  individual.taxon.canonical.name tag.local.identifier  \\\n",
       "0                    Papio anubis                10362   \n",
       "1                    Papio anubis                10362   \n",
       "2                    Papio anubis                10362   \n",
       "3                    Papio anubis                10362   \n",
       "4                    Papio anubis                10362   \n",
       "\n",
       "   individual.local.identifier                study.name  \\\n",
       "0                  24AB04_0V2Z  Baboons MBRP Mpala Kenya   \n",
       "1                  24AB04_0V2Z  Baboons MBRP Mpala Kenya   \n",
       "2                  24AB04_0V2Z  Baboons MBRP Mpala Kenya   \n",
       "3                  24AB04_0V2Z  Baboons MBRP Mpala Kenya   \n",
       "4                  24AB04_0V2Z  Baboons MBRP Mpala Kenya   \n",
       "\n",
       "            local_timestamp tag_local_identifier    group_id  \\\n",
       "0 2025-09-05 03:00:00+03:00                10362  Periwinkle   \n",
       "1 2025-09-05 03:01:00+03:00                10362  Periwinkle   \n",
       "2 2025-09-05 03:02:00+03:00                10362  Periwinkle   \n",
       "3 2025-09-05 03:03:00+03:00                10362  Periwinkle   \n",
       "4 2025-09-05 03:04:00+03:00                10362  Periwinkle   \n",
       "\n",
       "                                               x_cal  \\\n",
       "0  {'x1': 2319.0, 'x2': 2318.0, 'x3': 2318.0, 'x4...   \n",
       "1  {'x1': 2530.0, 'x2': 2532.0, 'x3': 2532.0, 'x4...   \n",
       "2  {'x1': 2524.0, 'x2': 2528.0, 'x3': 2529.0, 'x4...   \n",
       "3  {'x1': 2298.0, 'x2': 2300.0, 'x3': 2299.0, 'x4...   \n",
       "4  {'x1': 2286.0, 'x2': 2288.0, 'x3': 2288.0, 'x4...   \n",
       "\n",
       "                                               y_cal  \\\n",
       "0  {'y1': 2360.0, 'y2': 2360.0, 'y3': 2360.0, 'y4...   \n",
       "1  {'y1': 2123.0, 'y2': 2124.0, 'y3': 2122.0, 'y4...   \n",
       "2  {'y1': 2179.0, 'y2': 2162.0, 'y3': 2145.0, 'y4...   \n",
       "3  {'y1': 2411.0, 'y2': 2411.0, 'y3': 2411.0, 'y4...   \n",
       "4  {'y1': 2416.0, 'y2': 2415.0, 'y3': 2415.0, 'y4...   \n",
       "\n",
       "                                               z_cal  \n",
       "0  {'z1': 1744.0, 'z2': 1744.0, 'z3': 1744.0, 'z4...  \n",
       "1  {'z1': 1899.0, 'z2': 1900.0, 'z3': 1898.0, 'z4...  \n",
       "2  {'z1': 1897.0, 'z2': 1900.0, 'z3': 1903.0, 'z4...  \n",
       "3  {'z1': 1787.0, 'z2': 1788.0, 'z3': 1785.0, 'z4...  \n",
       "4  {'z1': 1782.0, 'z2': 1783.0, 'z3': 1782.0, 'z4...  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---- Main Processing ----\n",
    "# Load the data\n",
    "os.chdir('/mnt/EAS_ind/rharel/analysis/JK_ch1/')\n",
    "acc_data_trim = pd.read_parquet('data/acc_v1_char_sep2025_leopard.parquet')\n",
    "acc_data_trim.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb4cd7cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>tag</th>\n",
       "      <th>index</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-07-01 00:10:00+00:00</td>\n",
       "      <td>24AA11_9A7D</td>\n",
       "      <td>1</td>\n",
       "      <td>2.027727</td>\n",
       "      <td>-9.786848</td>\n",
       "      <td>1.172884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-07-01 00:10:00+00:00</td>\n",
       "      <td>24AA11_9A7D</td>\n",
       "      <td>2</td>\n",
       "      <td>1.970339</td>\n",
       "      <td>-9.979124</td>\n",
       "      <td>0.961380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-07-01 00:10:00+00:00</td>\n",
       "      <td>24AA11_9A7D</td>\n",
       "      <td>3</td>\n",
       "      <td>1.587749</td>\n",
       "      <td>-10.421359</td>\n",
       "      <td>0.403780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-07-01 00:10:00+00:00</td>\n",
       "      <td>24AA11_9A7D</td>\n",
       "      <td>4</td>\n",
       "      <td>1.128641</td>\n",
       "      <td>-6.845026</td>\n",
       "      <td>-0.269186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-07-01 00:10:00+00:00</td>\n",
       "      <td>24AA11_9A7D</td>\n",
       "      <td>5</td>\n",
       "      <td>2.429447</td>\n",
       "      <td>-9.806076</td>\n",
       "      <td>1.019063</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  timestamp          tag index         X          Y         Z\n",
       "0 2024-07-01 00:10:00+00:00  24AA11_9A7D     1  2.027727  -9.786848  1.172884\n",
       "1 2024-07-01 00:10:00+00:00  24AA11_9A7D     2  1.970339  -9.979124  0.961380\n",
       "2 2024-07-01 00:10:00+00:00  24AA11_9A7D     3  1.587749 -10.421359  0.403780\n",
       "3 2024-07-01 00:10:00+00:00  24AA11_9A7D     4  1.128641  -6.845026 -0.269186\n",
       "4 2024-07-01 00:10:00+00:00  24AA11_9A7D     5  2.429447  -9.806076  1.019063"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---- Main Processing ----\n",
    "# Load the data\n",
    "acc_data_trim = pd.read_parquet('data/acc_v0/24AA11_9A7D.parquet')\n",
    "acc_data_trim.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2d8a98f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['event.id', 'visible', 'timestamp', 'data.decoding.software',\n",
       "       'eobs.acceleration.axes',\n",
       "       'eobs.acceleration.sampling.frequency.per.axis',\n",
       "       'eobs.accelerations.raw', 'eobs.key.bin.checksum',\n",
       "       'eobs.start.timestamp', 'import.marked.outlier', 'sensor.type',\n",
       "       'individual.taxon.canonical.name', 'tag.local.identifier',\n",
       "       'individual.local.identifier', 'study.name', 'local_timestamp',\n",
       "       'tag_local_identifier', 'group_id', 'x_cal', 'y_cal', 'z_cal'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_data_trim.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e2629a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'tag'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/EAS_ind/rharel/analysis/JK_ch1/.venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3805\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3806\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'tag'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ---- Long Format Processing (Optimized) ----\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Create grouping column based on tag and timestamp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m acc_data_trim[\u001b[33m'\u001b[39m\u001b[33mgroup_id\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43macc_data_trim\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtag\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m + \u001b[33m'\u001b[39m\u001b[33m_\u001b[39m\u001b[33m'\u001b[39m + acc_data_trim[\u001b[33m'\u001b[39m\u001b[33mtimestamp\u001b[39m\u001b[33m'\u001b[39m].astype(\u001b[38;5;28mstr\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTotal number of groups (unique tag-timestamp combinations): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc_data_trim[\u001b[33m'\u001b[39m\u001b[33mgroup_id\u001b[39m\u001b[33m'\u001b[39m].nunique()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTotal number of rows: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(acc_data_trim)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/EAS_ind/rharel/analysis/JK_ch1/.venv/lib/python3.12/site-packages/pandas/core/frame.py:4102\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4102\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4104\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/EAS_ind/rharel/analysis/JK_ch1/.venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3807\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3808\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3809\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3810\u001b[39m     ):\n\u001b[32m   3811\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3814\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3815\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3816\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3817\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'tag'"
     ]
    }
   ],
   "source": [
    "# ---- Long Format Processing (Optimized) ----\n",
    "\n",
    "# Create grouping column based on tag and timestamp\n",
    "acc_data_trim['group_id'] = acc_data_trim['tag'] + '_' + acc_data_trim['timestamp'].astype(str)\n",
    "\n",
    "print(f\"Total number of groups (unique tag-timestamp combinations): {acc_data_trim['group_id'].nunique()}\")\n",
    "print(f\"Total number of rows: {len(acc_data_trim)}\")\n",
    "\n",
    "# Function to process each group (burst of accelerometer data)\n",
    "def process_group(group_data):\n",
    "    \"\"\"\n",
    "    Process a group (burst) of accelerometer data to calculate VEDBA metrics.\n",
    "    Each group represents all accelerometer readings for a specific tag-timestamp combination.\n",
    "    \"\"\"\n",
    "    # Extract X, Y, Z arrays for this group and convert to float\n",
    "    x_values = group_data['X'].values.astype(float)\n",
    "    y_values = group_data['Y'].values.astype(float)\n",
    "    z_values = group_data['Z'].values.astype(float)\n",
    "    \n",
    "    # Calculate dynamic acceleration components\n",
    "    x_component = np.abs(dy_acc(x_values))\n",
    "    y_component = np.abs(dy_acc(y_values))\n",
    "    z_component = np.abs(dy_acc(z_values))\n",
    "    \n",
    "    # Calculate vectorial sum (VEDBA for each sample)\n",
    "    vectorial_sum = np.sqrt(x_component**2 + y_component**2 + z_component**2)\n",
    "    ave_vedba_value = np.nanmean(vectorial_sum)\n",
    "    \n",
    "    # Calculate pitch\n",
    "    pitch = np.arctan2(x_component, np.sqrt(y_component**2 + z_component**2))\n",
    "    ave_pitch = np.nanmean(pitch)\n",
    "    \n",
    "    # Return summary metrics for this group\n",
    "    return pd.Series({\n",
    "        'tag': group_data['tag'].iloc[0],\n",
    "        'timestamp': group_data['timestamp'].iloc[0], \n",
    "        'ave_vedba': ave_vedba_value,\n",
    "        'ave_pitch': ave_pitch,\n",
    "        'n_samples': len(group_data)\n",
    "    })\n",
    "\n",
    "# Process groups with minimal output to avoid freezing\n",
    "print(\"Processing groups... (this will run silently to avoid output overflow)\")\n",
    "\n",
    "# Process all groups at once (no chunking for cleaner output)\n",
    "results_df = acc_data_trim.groupby('group_id', group_keys=False).apply(process_group).reset_index(drop=True)\n",
    "\n",
    "# Log-transform like in R\n",
    "results_df['log_vedba'] = np.log(results_df['ave_vedba'])\n",
    "\n",
    "# Add individual identifier columns\n",
    "results_df['individual_local_identifier'] = results_df['tag']\n",
    "results_df['tag_local_identifier'] = results_df['tag']\n",
    "results_df['local_timestamp'] = results_df['timestamp']\n",
    "\n",
    "# Select relevant columns\n",
    "filtered_data = results_df[['individual_local_identifier', \n",
    "                           'local_timestamp', \n",
    "                           'tag_local_identifier', \n",
    "                           'log_vedba', \n",
    "                           'ave_pitch',\n",
    "                           'n_samples']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aee2dacf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Successfully processed 466470 groups\n",
      "Sample size per group stats:\n",
      "  Mean: 40.0\n",
      "  Min: 40\n",
      "  Max: 40\n",
      "✓ Data saved to 'data/acc_vedba_long_format.parquet'\n",
      "\n",
      "First 3 rows of results:\n",
      "  individual_local_identifier           local_timestamp tag_local_identifier  \\\n",
      "0                 24AA11_9A7D 2024-07-01 00:10:00+00:00          24AA11_9A7D   \n",
      "1                 24AA11_9A7D 2024-07-01 00:11:00+00:00          24AA11_9A7D   \n",
      "2                 24AA11_9A7D 2024-07-01 00:32:00+00:00          24AA11_9A7D   \n",
      "\n",
      "   log_vedba  ave_pitch  n_samples  \n",
      "0   2.718889   0.672855         40  \n",
      "1   0.094771   0.631980         40  \n",
      "2   0.050509   0.477942         40  \n"
     ]
    }
   ],
   "source": [
    "print(f\"✓ Successfully processed {len(filtered_data)} groups\")\n",
    "print(f\"Sample size per group stats:\")\n",
    "print(f\"  Mean: {results_df['n_samples'].mean():.1f}\")\n",
    "print(f\"  Min: {results_df['n_samples'].min()}\")\n",
    "print(f\"  Max: {results_df['n_samples'].max()}\")\n",
    "\n",
    "# Save the processed data\n",
    "filtered_data.to_parquet('data/acc_vedba_long_format.parquet', index=False)\n",
    "print(\"✓ Data saved to 'data/acc_vedba_long_format.parquet'\")\n",
    "\n",
    "# Show just first few rows\n",
    "print(\"\\nFirst 3 rows of results:\")\n",
    "print(filtered_data.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b0db4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19 parquet files to process:\n",
      "  - 24AA01_5O8B.parquet\n",
      "  - 24AA03_2A1P.parquet\n",
      "  - 24AA05_4I0L.parquet\n",
      "  - 24AA06_5I8Y.parquet\n",
      "  - 24AA10_4R7W.parquet\n",
      "  - 24AA11_9A7D.parquet\n",
      "  - 24AA12_6P8Q.parquet\n",
      "  - 24AA14_4N0F.parquet\n",
      "  - 24AA16_9Q8P.parquet\n",
      "  - 24AB02_0Y5R.parquet\n",
      "  - 24AB03_4D7N.parquet\n",
      "  - 24AB04_0V2Z.parquet\n",
      "  - 24AB06_3I2H.parquet\n",
      "  - 24AB07_5J8U.parquet\n",
      "  - 24AC14_1D2E.parquet\n",
      "  - 24AC18_9L0M.parquet\n",
      "  - 24AC19_2N30.parquet\n",
      "  - 24AD08_0F1G.parquet\n",
      "  - 24AE08_4T5U.parquet\n",
      "\n",
      "==================================================\n",
      "PROCESSING ALL FILES\n",
      "==================================================\n",
      "Processing: 24AA01_5O8B.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rharel\\AppData\\Local\\Temp\\ipykernel_50468\\956589606.py:28: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  results_df = acc_data.groupby('group_id', group_keys=False).apply(process_group).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Processed 443232 groups from 24AA01_5O8B.parquet\n",
      "Processing: 24AA03_2A1P.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rharel\\AppData\\Local\\Temp\\ipykernel_50468\\956589606.py:28: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  results_df = acc_data.groupby('group_id', group_keys=False).apply(process_group).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Processed 435904 groups from 24AA03_2A1P.parquet\n",
      "Processing: 24AA05_4I0L.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rharel\\AppData\\Local\\Temp\\ipykernel_50468\\956589606.py:28: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  results_df = acc_data.groupby('group_id', group_keys=False).apply(process_group).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Processed 161671 groups from 24AA05_4I0L.parquet\n",
      "Processing: 24AA06_5I8Y.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rharel\\AppData\\Local\\Temp\\ipykernel_50468\\956589606.py:28: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  results_df = acc_data.groupby('group_id', group_keys=False).apply(process_group).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Processed 463650 groups from 24AA06_5I8Y.parquet\n",
      "Processing: 24AA10_4R7W.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rharel\\AppData\\Local\\Temp\\ipykernel_50468\\956589606.py:28: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  results_df = acc_data.groupby('group_id', group_keys=False).apply(process_group).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Processed 469463 groups from 24AA10_4R7W.parquet\n",
      "Processing: 24AA11_9A7D.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rharel\\AppData\\Local\\Temp\\ipykernel_50468\\956589606.py:28: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  results_df = acc_data.groupby('group_id', group_keys=False).apply(process_group).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Processed 466470 groups from 24AA11_9A7D.parquet\n",
      "Processing: 24AA12_6P8Q.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rharel\\AppData\\Local\\Temp\\ipykernel_50468\\956589606.py:28: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  results_df = acc_data.groupby('group_id', group_keys=False).apply(process_group).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Processed 464010 groups from 24AA12_6P8Q.parquet\n",
      "Processing: 24AA14_4N0F.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rharel\\AppData\\Local\\Temp\\ipykernel_50468\\956589606.py:28: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  results_df = acc_data.groupby('group_id', group_keys=False).apply(process_group).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Processed 459197 groups from 24AA14_4N0F.parquet\n",
      "Processing: 24AA16_9Q8P.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rharel\\AppData\\Local\\Temp\\ipykernel_50468\\956589606.py:28: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  results_df = acc_data.groupby('group_id', group_keys=False).apply(process_group).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Processed 473707 groups from 24AA16_9Q8P.parquet\n",
      "Processing: 24AB02_0Y5R.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rharel\\AppData\\Local\\Temp\\ipykernel_50468\\956589606.py:28: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  results_df = acc_data.groupby('group_id', group_keys=False).apply(process_group).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Processed 445905 groups from 24AB02_0Y5R.parquet\n",
      "Processing: 24AB03_4D7N.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rharel\\AppData\\Local\\Temp\\ipykernel_50468\\956589606.py:28: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  results_df = acc_data.groupby('group_id', group_keys=False).apply(process_group).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Processed 483582 groups from 24AB03_4D7N.parquet\n",
      "Processing: 24AB04_0V2Z.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rharel\\AppData\\Local\\Temp\\ipykernel_50468\\956589606.py:28: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  results_df = acc_data.groupby('group_id', group_keys=False).apply(process_group).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Processed 393064 groups from 24AB04_0V2Z.parquet\n",
      "Processing: 24AB06_3I2H.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rharel\\AppData\\Local\\Temp\\ipykernel_50468\\956589606.py:28: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  results_df = acc_data.groupby('group_id', group_keys=False).apply(process_group).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Processed 475211 groups from 24AB06_3I2H.parquet\n",
      "Processing: 24AB07_5J8U.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rharel\\AppData\\Local\\Temp\\ipykernel_50468\\956589606.py:28: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  results_df = acc_data.groupby('group_id', group_keys=False).apply(process_group).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Processed 464678 groups from 24AB07_5J8U.parquet\n",
      "Processing: 24AC14_1D2E.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rharel\\AppData\\Local\\Temp\\ipykernel_50468\\956589606.py:28: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  results_df = acc_data.groupby('group_id', group_keys=False).apply(process_group).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Processed 438257 groups from 24AC14_1D2E.parquet\n",
      "Processing: 24AC18_9L0M.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rharel\\AppData\\Local\\Temp\\ipykernel_50468\\956589606.py:28: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  results_df = acc_data.groupby('group_id', group_keys=False).apply(process_group).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Processed 444351 groups from 24AC18_9L0M.parquet\n",
      "Processing: 24AC19_2N30.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rharel\\AppData\\Local\\Temp\\ipykernel_50468\\956589606.py:28: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  results_df = acc_data.groupby('group_id', group_keys=False).apply(process_group).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Processed 279214 groups from 24AC19_2N30.parquet\n",
      "Processing: 24AD08_0F1G.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rharel\\AppData\\Local\\Temp\\ipykernel_50468\\956589606.py:28: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  results_df = acc_data.groupby('group_id', group_keys=False).apply(process_group).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Processed 350438 groups from 24AD08_0F1G.parquet\n",
      "Processing: 24AE08_4T5U.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rharel\\AppData\\Local\\Temp\\ipykernel_50468\\956589606.py:28: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  results_df = acc_data.groupby('group_id', group_keys=False).apply(process_group).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Processed 85873 groups from 24AE08_4T5U.parquet\n",
      "\n",
      "==================================================\n",
      "SUMMARY\n",
      "==================================================\n",
      "✓ Successfully processed 19 files\n",
      "✓ Total groups processed: 7697877\n",
      "✓ Combined dataset shape: (7697877, 6)\n",
      "\n",
      "==================================================\n",
      "SUMMARY\n",
      "==================================================\n",
      "✓ Successfully processed 19 files\n",
      "✓ Total groups processed: 7697877\n",
      "✓ Combined dataset shape: (7697877, 6)\n",
      "✓ Unique individuals: 19\n",
      "✓ Date range: 2024-07-01 00:00:00+00:00 to 2025-06-02 00:00:00+00:00\n",
      "✓ Unique individuals: 19\n",
      "✓ Date range: 2024-07-01 00:00:00+00:00 to 2025-06-02 00:00:00+00:00\n",
      "✓ Combined data saved to: data/all_acc_vedba_combined.parquet\n",
      "\n",
      "Sample size per group stats:\n",
      "  Mean: 40.0\n",
      "  Min: 40\n",
      "  Max: 40\n",
      "\n",
      "VEDBA statistics:\n",
      "  Mean log_vedba: 2.096\n",
      "✓ Combined data saved to: data/all_acc_vedba_combined.parquet\n",
      "\n",
      "Sample size per group stats:\n",
      "  Mean: 40.0\n",
      "  Min: 40\n",
      "  Max: 40\n",
      "\n",
      "VEDBA statistics:\n",
      "  Mean log_vedba: 2.096\n",
      "  Std log_vedba: 1.638\n",
      "\n",
      "First 5 rows of combined data:\n",
      "  individual_local_identifier           local_timestamp tag_local_identifier  \\\n",
      "0                 24AA01_5O8B 2024-07-01 00:00:00+00:00          24AA01_5O8B   \n",
      "1                 24AA01_5O8B 2024-07-01 00:01:00+00:00          24AA01_5O8B   \n",
      "2                 24AA01_5O8B 2024-07-01 00:02:00+00:00          24AA01_5O8B   \n",
      "3                 24AA01_5O8B 2024-07-01 00:03:00+00:00          24AA01_5O8B   \n",
      "4                 24AA01_5O8B 2024-07-01 00:04:00+00:00          24AA01_5O8B   \n",
      "\n",
      "   log_vedba  ave_pitch  n_samples  \n",
      "0   1.363207   0.594150         40  \n",
      "1   0.967197   0.556871         40  \n",
      "2   0.795673   0.500870         40  \n",
      "3   0.752691   0.691348         40  \n",
      "4   0.865714   0.645954         40  \n",
      "  Std log_vedba: 1.638\n",
      "\n",
      "First 5 rows of combined data:\n",
      "  individual_local_identifier           local_timestamp tag_local_identifier  \\\n",
      "0                 24AA01_5O8B 2024-07-01 00:00:00+00:00          24AA01_5O8B   \n",
      "1                 24AA01_5O8B 2024-07-01 00:01:00+00:00          24AA01_5O8B   \n",
      "2                 24AA01_5O8B 2024-07-01 00:02:00+00:00          24AA01_5O8B   \n",
      "3                 24AA01_5O8B 2024-07-01 00:03:00+00:00          24AA01_5O8B   \n",
      "4                 24AA01_5O8B 2024-07-01 00:04:00+00:00          24AA01_5O8B   \n",
      "\n",
      "   log_vedba  ave_pitch  n_samples  \n",
      "0   1.363207   0.594150         40  \n",
      "1   0.967197   0.556871         40  \n",
      "2   0.795673   0.500870         40  \n",
      "3   0.752691   0.691348         40  \n",
      "4   0.865714   0.645954         40  \n"
     ]
    }
   ],
   "source": [
    "# ---- Process ALL Files and Combine ----\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Get all parquet files in the acc_v0 directory\n",
    "data_folder = 'data/acc_v0'\n",
    "parquet_files = glob.glob(os.path.join(data_folder, '*.parquet'))\n",
    "\n",
    "print(f\"Found {len(parquet_files)} parquet files to process:\")\n",
    "for file in parquet_files:\n",
    "    print(f\"  - {os.path.basename(file)}\")\n",
    "\n",
    "# Function to process a single file\n",
    "def process_single_file(file_path):\n",
    "    \"\"\"\n",
    "    Process a single parquet file and return VEDBA results\n",
    "    \"\"\"\n",
    "    print(f\"Processing: {os.path.basename(file_path)}\")\n",
    "    \n",
    "    # Load the data\n",
    "    acc_data = pd.read_parquet(file_path)\n",
    "    \n",
    "    # Create grouping column based on tag and timestamp\n",
    "    acc_data['group_id'] = acc_data['tag'] + '_' + acc_data['timestamp'].astype(str)\n",
    "    \n",
    "    # Process all groups\n",
    "    results_df = acc_data.groupby('group_id', group_keys=False).apply(process_group).reset_index(drop=True)\n",
    "    \n",
    "    # Log-transform\n",
    "    results_df['log_vedba'] = np.log(results_df['ave_vedba'])\n",
    "    \n",
    "    # Add individual identifier columns\n",
    "    results_df['individual_local_identifier'] = results_df['tag']\n",
    "    results_df['tag_local_identifier'] = results_df['tag']\n",
    "    results_df['local_timestamp'] = results_df['timestamp']\n",
    "    \n",
    "    # Select relevant columns\n",
    "    filtered_data = results_df[['individual_local_identifier', \n",
    "                               'local_timestamp', \n",
    "                               'tag_local_identifier', \n",
    "                               'log_vedba', \n",
    "                               'ave_pitch',\n",
    "                               'n_samples']]\n",
    "    \n",
    "    print(f\"  ✓ Processed {len(filtered_data)} groups from {os.path.basename(file_path)}\")\n",
    "    return filtered_data\n",
    "\n",
    "# Process all files and combine\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PROCESSING ALL FILES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "all_results = []\n",
    "total_groups = 0\n",
    "\n",
    "for file_path in parquet_files:\n",
    "    try:\n",
    "        file_results = process_single_file(file_path)\n",
    "        all_results.append(file_results)\n",
    "        total_groups += len(file_results)\n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ Error processing {os.path.basename(file_path)}: {str(e)}\")\n",
    "\n",
    "# Combine all results into one large dataframe\n",
    "if all_results:\n",
    "    combined_data = pd.concat(all_results, ignore_index=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"SUMMARY\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"✓ Successfully processed {len(parquet_files)} files\")\n",
    "    print(f\"✓ Total groups processed: {total_groups}\")\n",
    "    print(f\"✓ Combined dataset shape: {combined_data.shape}\")\n",
    "    print(f\"✓ Unique individuals: {combined_data['individual_local_identifier'].nunique()}\")\n",
    "    print(f\"✓ Date range: {combined_data['local_timestamp'].min()} to {combined_data['local_timestamp'].max()}\")\n",
    "    \n",
    "    # Save the combined data\n",
    "    output_file = 'data/all_acc_vedba_combined.parquet'\n",
    "    combined_data.to_parquet(output_file, index=False)\n",
    "    print(f\"✓ Combined data saved to: {output_file}\")\n",
    "    \n",
    "    # Show summary statistics\n",
    "    print(f\"\\nSample size per group stats:\")\n",
    "    print(f\"  Mean: {combined_data['n_samples'].mean():.1f}\")\n",
    "    print(f\"  Min: {combined_data['n_samples'].min()}\")\n",
    "    print(f\"  Max: {combined_data['n_samples'].max()}\")\n",
    "    \n",
    "    print(f\"\\nVEDBA statistics:\")\n",
    "    print(f\"  Mean log_vedba: {combined_data['log_vedba'].mean():.3f}\")\n",
    "    print(f\"  Std log_vedba: {combined_data['log_vedba'].std():.3f}\")\n",
    "    \n",
    "    # Show first few rows\n",
    "    print(f\"\\nFirst 5 rows of combined data:\")\n",
    "    print(combined_data.head())\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No files were successfully processed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c94b681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 96 files (VEDBA only)...\n",
      "[1/96] 24AC20_4M5N.parquet\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 47\u001b[39m\n\u001b[32m     44\u001b[39m     acc_data[\u001b[33m'\u001b[39m\u001b[33mgroup_id\u001b[39m\u001b[33m'\u001b[39m] = acc_data[\u001b[33m'\u001b[39m\u001b[33mtag\u001b[39m\u001b[33m'\u001b[39m] + \u001b[33m'\u001b[39m\u001b[33m_\u001b[39m\u001b[33m'\u001b[39m + acc_data[\u001b[33m'\u001b[39m\u001b[33mtimestamp\u001b[39m\u001b[33m'\u001b[39m].astype(\u001b[38;5;28mstr\u001b[39m)\n\u001b[32m     46\u001b[39m     \u001b[38;5;66;03m# Process groups\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     results = \u001b[43macc_data\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mgroup_id\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_group_simple\u001b[49m\u001b[43m)\u001b[49m.reset_index(drop=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     48\u001b[39m     all_results.append(results)\n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m# Combine and finalize\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/EAS_ind/rharel/analysis/JK_ch1/.venv/lib/python3.12/site-packages/pandas/core/groupby/groupby.py:1824\u001b[39m, in \u001b[36mGroupBy.apply\u001b[39m\u001b[34m(self, func, include_groups, *args, **kwargs)\u001b[39m\n\u001b[32m   1822\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[33m\"\u001b[39m\u001b[33mmode.chained_assignment\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   1823\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1824\u001b[39m         result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_python_apply_general\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selected_obj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1825\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1826\u001b[39m             \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.obj, Series)\n\u001b[32m   1827\u001b[39m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._selection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1828\u001b[39m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._selected_obj.shape != \u001b[38;5;28mself\u001b[39m._obj_with_exclusions.shape\n\u001b[32m   1829\u001b[39m         ):\n\u001b[32m   1830\u001b[39m             warnings.warn(\n\u001b[32m   1831\u001b[39m                 message=_apply_groupings_depr.format(\n\u001b[32m   1832\u001b[39m                     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mapply\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1835\u001b[39m                 stacklevel=find_stack_level(),\n\u001b[32m   1836\u001b[39m             )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/EAS_ind/rharel/analysis/JK_ch1/.venv/lib/python3.12/site-packages/pandas/core/groupby/groupby.py:1885\u001b[39m, in \u001b[36mGroupBy._python_apply_general\u001b[39m\u001b[34m(self, f, data, not_indexed_same, is_transform, is_agg)\u001b[39m\n\u001b[32m   1850\u001b[39m \u001b[38;5;129m@final\u001b[39m\n\u001b[32m   1851\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_python_apply_general\u001b[39m(\n\u001b[32m   1852\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1857\u001b[39m     is_agg: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   1858\u001b[39m ) -> NDFrameT:\n\u001b[32m   1859\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1860\u001b[39m \u001b[33;03m    Apply function f in python space\u001b[39;00m\n\u001b[32m   1861\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1883\u001b[39m \u001b[33;03m        data after applying f\u001b[39;00m\n\u001b[32m   1884\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1885\u001b[39m     values, mutated = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_grouper\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply_groupwise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1886\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m not_indexed_same \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1887\u001b[39m         not_indexed_same = mutated\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/EAS_ind/rharel/analysis/JK_ch1/.venv/lib/python3.12/site-packages/pandas/core/groupby/ops.py:919\u001b[39m, in \u001b[36mBaseGrouper.apply_groupwise\u001b[39m\u001b[34m(self, f, data, axis)\u001b[39m\n\u001b[32m    917\u001b[39m \u001b[38;5;66;03m# group might be modified\u001b[39;00m\n\u001b[32m    918\u001b[39m group_axes = group.axes\n\u001b[32m--> \u001b[39m\u001b[32m919\u001b[39m res = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mutated \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_indexed_like(res, group_axes, axis):\n\u001b[32m    921\u001b[39m     mutated = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mprocess_group_simple\u001b[39m\u001b[34m(group_data)\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Calculate VEDBA components\u001b[39;00m\n\u001b[32m     17\u001b[39m x_component = np.abs(dy_acc(x_values))\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m y_component = np.abs(\u001b[43mdy_acc\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_values\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     19\u001b[39m z_component = np.abs(dy_acc(z_values))\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# VEDBA calculation\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 40\u001b[39m, in \u001b[36mdy_acc\u001b[39m\u001b[34m(vect, win_size)\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(vect)):\n\u001b[32m     39\u001b[39m     window = padded[i : i + (\u001b[32m2\u001b[39m * pad_size + \u001b[32m1\u001b[39m)]\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     m_ave = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnanmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m     acc_vec[i] = vect[i] - m_ave\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m acc_vec\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/EAS_ind/rharel/analysis/JK_ch1/.venv/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1052\u001b[39m, in \u001b[36mnanmean\u001b[39m\u001b[34m(a, axis, dtype, out, keepdims, where)\u001b[39m\n\u001b[32m   1049\u001b[39m avg = _divide_by_count(tot, cnt, out=out)\n\u001b[32m   1051\u001b[39m isbad = (cnt == \u001b[32m0\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1052\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43misbad\u001b[49m\u001b[43m.\u001b[49m\u001b[43many\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   1053\u001b[39m     warnings.warn(\u001b[33m\"\u001b[39m\u001b[33mMean of empty slice\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;167;01mRuntimeWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m   1054\u001b[39m     \u001b[38;5;66;03m# NaN is the only possible bad value, so no further\u001b[39;00m\n\u001b[32m   1055\u001b[39m     \u001b[38;5;66;03m# action is needed to handle bad results.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/EAS_ind/rharel/analysis/JK_ch1/.venv/lib/python3.12/site-packages/numpy/_core/_methods.py:64\u001b[39m, in \u001b[36m_any\u001b[39m\u001b[34m(a, axis, dtype, out, keepdims, where)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# Parsing keyword arguments is currently fairly slow, so avoid it for now\u001b[39;00m\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m where \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mumr_any\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m umr_any(a, axis, dtype, out, keepdims, where=where)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ---- SIMPLIFIED: Process ALL Files - VEDBA Only ----\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Simplified function to process each group (VEDBA only)\n",
    "def process_group_simple(group_data):\n",
    "    \"\"\"\n",
    "    Simplified processing: only calculate VEDBA metrics\n",
    "    \"\"\"\n",
    "    # Convert to float and calculate dynamic acceleration\n",
    "    x_values = group_data['X'].values.astype(float)\n",
    "    y_values = group_data['Y'].values.astype(float)\n",
    "    z_values = group_data['Z'].values.astype(float)\n",
    "    \n",
    "    # Calculate VEDBA components\n",
    "    x_component = np.abs(dy_acc(x_values))\n",
    "    y_component = np.abs(dy_acc(y_values))\n",
    "    z_component = np.abs(dy_acc(z_values))\n",
    "    \n",
    "    # VEDBA calculation\n",
    "    vectorial_sum = np.sqrt(x_component**2 + y_component**2 + z_component**2)\n",
    "    ave_vedba = np.nanmean(vectorial_sum)\n",
    "    \n",
    "    return pd.Series({\n",
    "        'tag': group_data['tag'].iloc[0],\n",
    "        'timestamp': group_data['timestamp'].iloc[0], \n",
    "        'ave_vedba': ave_vedba,\n",
    "        'n_samples': len(group_data)\n",
    "    })\n",
    "\n",
    "# Process all files\n",
    "data_folder = 'data/acc_v0'\n",
    "parquet_files = glob.glob(os.path.join(data_folder, '*.parquet'))\n",
    "\n",
    "print(f\"Processing {len(parquet_files)} files (VEDBA only)...\")\n",
    "\n",
    "all_results = []\n",
    "for i, file_path in enumerate(parquet_files, 1):\n",
    "    print(f\"[{i}/{len(parquet_files)}] {os.path.basename(file_path)}\")\n",
    "    \n",
    "    # Load and process\n",
    "    acc_data = pd.read_parquet(file_path)\n",
    "    acc_data['group_id'] = acc_data['tag'] + '_' + acc_data['timestamp'].astype(str)\n",
    "    \n",
    "    # Process groups\n",
    "    results = acc_data.groupby('group_id', group_keys=False).apply(process_group_simple).reset_index(drop=True)\n",
    "    all_results.append(results)\n",
    "\n",
    "# Combine and finalize\n",
    "combined_data = pd.concat(all_results, ignore_index=True)\n",
    "\n",
    "# Add log transformation\n",
    "combined_data['log_vedba'] = np.log(combined_data['ave_vedba'])\n",
    "\n",
    "# Final dataset with only essential columns\n",
    "final_data = combined_data[['tag', 'timestamp', 'ave_vedba', 'log_vedba', 'n_samples']]\n",
    "\n",
    "print(f\"\\n✓ Processed {len(final_data)} total groups from {len(parquet_files)} files\")\n",
    "print(f\"✓ Dataset shape: {final_data.shape}\")\n",
    "print(f\"✓ Unique tags: {final_data['tag'].nunique()}\")\n",
    "\n",
    "# Save simplified result\n",
    "final_data.to_parquet('data/vedba_simplified_all.parquet', index=False)\n",
    "print(f\"✓ Saved to: data/vedba_simplified_all.parquet\")\n",
    "\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "print(final_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa6e1468",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/vedba_simplified_all.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m     vedba_data = final_data.copy()\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     13\u001b[39m     \u001b[38;5;66;03m# If final_data doesn't exist, load from saved file\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     vedba_data = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdata/vedba_simplified_all.parquet\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCreating separate files for each animal...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTotal data shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvedba_data.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parquet.py:669\u001b[39m, in \u001b[36mread_parquet\u001b[39m\u001b[34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[39m\n\u001b[32m    666\u001b[39m     use_nullable_dtypes = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    667\u001b[39m check_dtype_backend(dtype_backend)\n\u001b[32m--> \u001b[39m\u001b[32m669\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parquet.py:258\u001b[39m, in \u001b[36mPyArrowImpl.read\u001b[39m\u001b[34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[39m\n\u001b[32m    256\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m manager == \u001b[33m\"\u001b[39m\u001b[33marray\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    257\u001b[39m     to_pandas_kwargs[\u001b[33m\"\u001b[39m\u001b[33msplit_blocks\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m path_or_handle, handles, filesystem = \u001b[43m_get_path_or_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    265\u001b[39m     pa_table = \u001b[38;5;28mself\u001b[39m.api.parquet.read_table(\n\u001b[32m    266\u001b[39m         path_or_handle,\n\u001b[32m    267\u001b[39m         columns=columns,\n\u001b[32m   (...)\u001b[39m\u001b[32m    270\u001b[39m         **kwargs,\n\u001b[32m    271\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parquet.py:141\u001b[39m, in \u001b[36m_get_path_or_handle\u001b[39m\u001b[34m(path, fs, storage_options, mode, is_dir)\u001b[39m\n\u001b[32m    131\u001b[39m handles = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    133\u001b[39m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[32m    134\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[32m   (...)\u001b[39m\u001b[32m    139\u001b[39m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[32m    140\u001b[39m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m     handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    144\u001b[39m     fs = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    145\u001b[39m     path_or_handle = handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\common.py:882\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    873\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(\n\u001b[32m    874\u001b[39m             handle,\n\u001b[32m    875\u001b[39m             ioargs.mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m    878\u001b[39m             newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    879\u001b[39m         )\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n\u001b[32m    883\u001b[39m     handles.append(handle)\n\u001b[32m    885\u001b[39m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'data/vedba_simplified_all.parquet'"
     ]
    }
   ],
   "source": [
    "# ---- Create Separate VEDBA Files by Animal ----\n",
    "\n",
    "import os\n",
    "\n",
    "# Create the vedba directory if it doesn't exist\n",
    "vedba_dir = 'data/vedba'\n",
    "os.makedirs(vedba_dir, exist_ok=True)\n",
    "\n",
    "# Load the combined VEDBA data (assuming it exists from previous cell)\n",
    "if 'final_data' in locals():\n",
    "    vedba_data = final_data.copy()\n",
    "else:\n",
    "    # If final_data doesn't exist, load from saved file\n",
    "    vedba_data = pd.read_parquet('data/vedba_simplified_all.parquet')\n",
    "\n",
    "print(f\"Creating separate files for each animal...\")\n",
    "print(f\"Total data shape: {vedba_data.shape}\")\n",
    "print(f\"Unique animals: {vedba_data['tag'].nunique()}\")\n",
    "\n",
    "# Get unique tags (animal IDs)\n",
    "unique_tags = vedba_data['tag'].unique()\n",
    "\n",
    "# Create separate file for each animal\n",
    "for i, tag in enumerate(unique_tags, 1):\n",
    "    # Filter data for this animal\n",
    "    animal_data = vedba_data[vedba_data['tag'] == tag].copy()\n",
    "    \n",
    "    # Sort by timestamp for chronological order\n",
    "    animal_data = animal_data.sort_values('timestamp').reset_index(drop=True)\n",
    "    \n",
    "    # Create filename\n",
    "    filename = f\"{tag}.parquet\"\n",
    "    filepath = os.path.join(vedba_dir, filename)\n",
    "    \n",
    "    # Save the file\n",
    "    animal_data.to_parquet(filepath, index=False)\n",
    "    \n",
    "    print(f\"[{i}/{len(unique_tags)}] {tag}: {len(animal_data)} records → {filename}\")\n",
    "\n",
    "print(f\"\\n✓ Created {len(unique_tags)} individual VEDBA files in '{vedba_dir}/' folder\")\n",
    "\n",
    "# Show summary of what was created\n",
    "print(f\"\\nFile summary:\")\n",
    "for tag in sorted(unique_tags):\n",
    "    animal_data = vedba_data[vedba_data['tag'] == tag]\n",
    "    date_range = f\"{animal_data['timestamp'].min()} to {animal_data['timestamp'].max()}\"\n",
    "    print(f\"  {tag}.parquet: {len(animal_data)} records, {date_range}\")\n",
    "\n",
    "print(f\"\\nFolder contents:\")\n",
    "vedba_files = os.listdir(vedba_dir)\n",
    "print(f\"  {len(vedba_files)} files created in data/vedba/\")\n",
    "for file in sorted(vedba_files)[:5]:  # Show first 5 files\n",
    "    print(f\"    - {file}\")\n",
    "if len(vedba_files) > 5:\n",
    "    print(f\"    ... and {len(vedba_files) - 5} more files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "189945a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 96 animal files individually...\n",
      "Each input file = 1 animal → 1 output VEDBA file\n",
      "\n",
      "============================================================\n",
      "PROCESSING INDIVIDUAL ANIMALS\n",
      "============================================================\n",
      "Processing: 24AA01_5O8B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rharel\\AppData\\Local\\Temp\\ipykernel_50468\\3606407724.py:59: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  results = acc_data.groupby('group_id', group_keys=False).apply(process_group_simple).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/96] 24AA01_5O8B: 443232 VEDBA records → 24AA01_5O8B.parquet\n",
      "Processing: 24AA03_2A1P\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 93\u001b[39m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, file_path \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(parquet_files, \u001b[32m1\u001b[39m):\n\u001b[32m     92\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m         result_summary = \u001b[43mprocess_animal_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     94\u001b[39m         summary_list.append(result_summary)\n\u001b[32m     96\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(parquet_files)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult_summary[\u001b[33m'\u001b[39m\u001b[33manimal_id\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     97\u001b[39m               \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult_summary[\u001b[33m'\u001b[39m\u001b[33mrecords\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m VEDBA records → \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult_summary[\u001b[33m'\u001b[39m\u001b[33moutput_file\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 59\u001b[39m, in \u001b[36mprocess_animal_file\u001b[39m\u001b[34m(file_path)\u001b[39m\n\u001b[32m     56\u001b[39m acc_data[\u001b[33m'\u001b[39m\u001b[33mgroup_id\u001b[39m\u001b[33m'\u001b[39m] = acc_data[\u001b[33m'\u001b[39m\u001b[33mtag\u001b[39m\u001b[33m'\u001b[39m] + \u001b[33m'\u001b[39m\u001b[33m_\u001b[39m\u001b[33m'\u001b[39m + acc_data[\u001b[33m'\u001b[39m\u001b[33mtimestamp\u001b[39m\u001b[33m'\u001b[39m].astype(\u001b[38;5;28mstr\u001b[39m)\n\u001b[32m     58\u001b[39m \u001b[38;5;66;03m# Process all groups for this animal\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m results = \u001b[43macc_data\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mgroup_id\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_group_simple\u001b[49m\u001b[43m)\u001b[49m.reset_index(drop=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# Add log transformation\u001b[39;00m\n\u001b[32m     62\u001b[39m results[\u001b[33m'\u001b[39m\u001b[33mlog_vedba\u001b[39m\u001b[33m'\u001b[39m] = np.log(results[\u001b[33m'\u001b[39m\u001b[33mave_vedba\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\groupby\\groupby.py:1824\u001b[39m, in \u001b[36mGroupBy.apply\u001b[39m\u001b[34m(self, func, include_groups, *args, **kwargs)\u001b[39m\n\u001b[32m   1822\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[33m\"\u001b[39m\u001b[33mmode.chained_assignment\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   1823\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1824\u001b[39m         result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_python_apply_general\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selected_obj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1825\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1826\u001b[39m             \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.obj, Series)\n\u001b[32m   1827\u001b[39m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._selection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1828\u001b[39m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._selected_obj.shape != \u001b[38;5;28mself\u001b[39m._obj_with_exclusions.shape\n\u001b[32m   1829\u001b[39m         ):\n\u001b[32m   1830\u001b[39m             warnings.warn(\n\u001b[32m   1831\u001b[39m                 message=_apply_groupings_depr.format(\n\u001b[32m   1832\u001b[39m                     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mapply\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1835\u001b[39m                 stacklevel=find_stack_level(),\n\u001b[32m   1836\u001b[39m             )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\groupby\\groupby.py:1885\u001b[39m, in \u001b[36mGroupBy._python_apply_general\u001b[39m\u001b[34m(self, f, data, not_indexed_same, is_transform, is_agg)\u001b[39m\n\u001b[32m   1850\u001b[39m \u001b[38;5;129m@final\u001b[39m\n\u001b[32m   1851\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_python_apply_general\u001b[39m(\n\u001b[32m   1852\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1857\u001b[39m     is_agg: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   1858\u001b[39m ) -> NDFrameT:\n\u001b[32m   1859\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1860\u001b[39m \u001b[33;03m    Apply function f in python space\u001b[39;00m\n\u001b[32m   1861\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1883\u001b[39m \u001b[33;03m        data after applying f\u001b[39;00m\n\u001b[32m   1884\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1885\u001b[39m     values, mutated = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_grouper\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply_groupwise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1886\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m not_indexed_same \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1887\u001b[39m         not_indexed_same = mutated\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\groupby\\ops.py:919\u001b[39m, in \u001b[36mBaseGrouper.apply_groupwise\u001b[39m\u001b[34m(self, f, data, axis)\u001b[39m\n\u001b[32m    917\u001b[39m \u001b[38;5;66;03m# group might be modified\u001b[39;00m\n\u001b[32m    918\u001b[39m group_axes = group.axes\n\u001b[32m--> \u001b[39m\u001b[32m919\u001b[39m res = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mutated \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_indexed_like(res, group_axes, axis):\n\u001b[32m    921\u001b[39m     mutated = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mprocess_group_simple\u001b[39m\u001b[34m(group_data)\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Calculate VEDBA components\u001b[39;00m\n\u001b[32m     28\u001b[39m x_component = np.abs(dy_acc(x_values))\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m y_component = np.abs(\u001b[43mdy_acc\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_values\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     30\u001b[39m z_component = np.abs(dy_acc(z_values))\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# VEDBA calculation\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 40\u001b[39m, in \u001b[36mdy_acc\u001b[39m\u001b[34m(vect, win_size)\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(vect)):\n\u001b[32m     39\u001b[39m     window = padded[i : i + (\u001b[32m2\u001b[39m * pad_size + \u001b[32m1\u001b[39m)]\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     m_ave = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnanmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m     acc_vec[i] = vect[i] - m_ave\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m acc_vec\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\lib\\_nanfunctions_impl.py:1046\u001b[39m, in \u001b[36mnanmean\u001b[39m\u001b[34m(a, axis, dtype, out, keepdims, where)\u001b[39m\n\u001b[32m   1042\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mIf a is inexact, then out must be inexact\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1044\u001b[39m cnt = np.sum(~mask, axis=axis, dtype=np.intp, keepdims=keepdims,\n\u001b[32m   1045\u001b[39m              where=where)\n\u001b[32m-> \u001b[39m\u001b[32m1046\u001b[39m tot = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1047\u001b[39m \u001b[43m             \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1048\u001b[39m avg = _divide_by_count(tot, cnt, out=out)\n\u001b[32m   1050\u001b[39m isbad = (cnt == \u001b[32m0\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ---- Process Each Animal File Individually ----\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Create the vedba directory if it doesn't exist\n",
    "vedba_dir = 'data/vedba'\n",
    "os.makedirs(vedba_dir, exist_ok=True)\n",
    "\n",
    "# Get all input parquet files (each represents one animal)\n",
    "data_folder = 'data/acc_v0'\n",
    "parquet_files = glob.glob(os.path.join(data_folder, '*.parquet'))\n",
    "\n",
    "print(f\"Processing {len(parquet_files)} animal files individually...\")\n",
    "print(\"Each input file = 1 animal → 1 output VEDBA file\")\n",
    "\n",
    "# Simplified function to process each group (VEDBA only)\n",
    "def process_group_simple(group_data):\n",
    "    \"\"\"\n",
    "    Simplified processing: only calculate VEDBA metrics\n",
    "    \"\"\"\n",
    "    # Convert to float and calculate dynamic acceleration\n",
    "    x_values = group_data['X'].values.astype(float)\n",
    "    y_values = group_data['Y'].values.astype(float)\n",
    "    z_values = group_data['Z'].values.astype(float)\n",
    "    \n",
    "    # Calculate VEDBA components\n",
    "    x_component = np.abs(dy_acc(x_values))\n",
    "    y_component = np.abs(dy_acc(y_values))\n",
    "    z_component = np.abs(dy_acc(z_values))\n",
    "    \n",
    "    # VEDBA calculation\n",
    "    vectorial_sum = np.sqrt(x_component**2 + y_component**2 + z_component**2)\n",
    "    ave_vedba = np.nanmean(vectorial_sum)\n",
    "    \n",
    "    return pd.Series({\n",
    "        'tag': group_data['tag'].iloc[0],\n",
    "        'timestamp': group_data['timestamp'].iloc[0], \n",
    "        'ave_vedba': ave_vedba,\n",
    "        'n_samples': len(group_data)\n",
    "    })\n",
    "\n",
    "def process_animal_file(file_path):\n",
    "    \"\"\"\n",
    "    Process a single animal file and create individual VEDBA output\n",
    "    \"\"\"\n",
    "    filename = os.path.basename(file_path)\n",
    "    animal_id = filename.replace('.parquet', '')  # Extract animal ID from filename\n",
    "    \n",
    "    print(f\"Processing: {animal_id}\")\n",
    "    \n",
    "    # Load the animal's accelerometer data\n",
    "    acc_data = pd.read_parquet(file_path)\n",
    "    \n",
    "    # Create grouping column for this animal's data\n",
    "    acc_data['group_id'] = acc_data['tag'] + '_' + acc_data['timestamp'].astype(str)\n",
    "    \n",
    "    # Process all groups for this animal\n",
    "    results = acc_data.groupby('group_id', group_keys=False).apply(process_group_simple).reset_index(drop=True)\n",
    "    \n",
    "    # Add log transformation\n",
    "    results['log_vedba'] = np.log(results['ave_vedba'])\n",
    "    \n",
    "    # Sort by timestamp for chronological order\n",
    "    results = results.sort_values('timestamp').reset_index(drop=True)\n",
    "    \n",
    "    # Select final columns\n",
    "    vedba_data = results[['tag', 'timestamp', 'ave_vedba', 'log_vedba', 'n_samples']]\n",
    "    \n",
    "    # Create output filename using the same animal ID\n",
    "    output_file = f\"{animal_id}.parquet\"\n",
    "    output_path = os.path.join(vedba_dir, output_file)\n",
    "    \n",
    "    # Save individual VEDBA file\n",
    "    vedba_data.to_parquet(output_path, index=False)\n",
    "    \n",
    "    return {\n",
    "        'animal_id': animal_id,\n",
    "        'input_file': filename,\n",
    "        'output_file': output_file,\n",
    "        'records': len(vedba_data),\n",
    "        'date_range': f\"{vedba_data['timestamp'].min()} to {vedba_data['timestamp'].max()}\"\n",
    "    }\n",
    "\n",
    "# Process each animal file\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PROCESSING INDIVIDUAL ANIMALS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "summary_list = []\n",
    "for i, file_path in enumerate(parquet_files, 1):\n",
    "    try:\n",
    "        result_summary = process_animal_file(file_path)\n",
    "        summary_list.append(result_summary)\n",
    "        \n",
    "        print(f\"[{i}/{len(parquet_files)}] {result_summary['animal_id']}: \"\n",
    "              f\"{result_summary['records']} VEDBA records → {result_summary['output_file']}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"[{i}/{len(parquet_files)}] ❌ Error processing {os.path.basename(file_path)}: {str(e)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"✓ Successfully processed {len(summary_list)} animals\")\n",
    "print(f\"✓ Created {len(summary_list)} individual VEDBA files in '{vedba_dir}/'\")\n",
    "\n",
    "# Show detailed summary\n",
    "print(f\"\\nDetailed summary:\")\n",
    "for summary in summary_list:\n",
    "    print(f\"  {summary['animal_id']}: {summary['records']} records\")\n",
    "    print(f\"    Date range: {summary['date_range']}\")\n",
    "    print(f\"    File: {summary['output_file']}\")\n",
    "    print()\n",
    "\n",
    "# Verify folder contents\n",
    "vedba_files = [f for f in os.listdir(vedba_dir) if f.endswith('.parquet')]\n",
    "print(f\"Folder verification:\")\n",
    "print(f\"  {len(vedba_files)} VEDBA files created in data/vedba/\")\n",
    "print(f\"  Files: {', '.join(sorted(vedba_files))}\")\n",
    "\n",
    "print(f\"\\n✓ Individual animal processing complete!\")\n",
    "print(f\"Each animal now has its own VEDBA file: data/vedba/[AnimalID].parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465880ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 PARALLEL PROCESSING SETUP\n",
      "📊 Found 96 animal files to process\n",
      "💻 Available CPU cores: 104\n",
      "⚡ Using 78 parallel workers\n",
      "📁 Output directory: data/vedba\n",
      "\n",
      "============================================================\n",
      "STARTING PARALLEL PROCESSING\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3050404/4180222515.py:70: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  results = acc_data.groupby('group_id', group_keys=False).apply(process_group_simple_parallel).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [1/96] 24AE47_3G6U: 6033 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3050404/4180222515.py:70: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  results = acc_data.groupby('group_id', group_keys=False).apply(process_group_simple_parallel).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [2/96] 24AE53_5D3D: 40262 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3050404/4180222515.py:70: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  results = acc_data.groupby('group_id', group_keys=False).apply(process_group_simple_parallel).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [3/96] 24AE40_0E0K: 48591 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3050404/4180222515.py:70: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  results = acc_data.groupby('group_id', group_keys=False).apply(process_group_simple_parallel).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [4/96] 24AE08_4T5U: 85873 records\n"
     ]
    }
   ],
   "source": [
    "# ---- PARALLEL Processing: Process ALL Files Much Faster ----\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import time\n",
    "\n",
    "# Create the vedba directory if it doesn't exist\n",
    "vedba_dir = 'data/vedba'\n",
    "os.makedirs(vedba_dir, exist_ok=True)\n",
    "\n",
    "def process_animal_file_parallel(file_path):\n",
    "    \"\"\"\n",
    "    Process a single animal file and create individual VEDBA output (parallel version)\n",
    "    This function will be run in separate processes\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import os\n",
    "    \n",
    "    # Re-define dy_acc function for parallel processing (each process needs its own copy)\n",
    "    def dy_acc(vect, win_size=7):\n",
    "        if vect is None or len(vect) == 0:\n",
    "            raise ValueError(\"Input vector is empty or invalid.\")\n",
    "        \n",
    "        pad_size = int(win_size / 2 - 0.5)\n",
    "        padded = np.pad(vect, (pad_size, pad_size), constant_values=np.nan)\n",
    "        acc_vec = np.empty(len(vect))\n",
    "        acc_vec[:] = np.nan\n",
    "\n",
    "        for i in range(len(vect)):\n",
    "            window = padded[i : i + (2 * pad_size + 1)]\n",
    "            m_ave = np.nanmean(window)\n",
    "            acc_vec[i] = vect[i] - m_ave\n",
    "        \n",
    "        return acc_vec\n",
    "    \n",
    "    def process_group_simple_parallel(group_data):\n",
    "        \"\"\"Simplified processing for parallel execution\"\"\"\n",
    "        x_values = group_data['X'].values.astype(float)\n",
    "        y_values = group_data['Y'].values.astype(float)\n",
    "        z_values = group_data['Z'].values.astype(float)\n",
    "        \n",
    "        x_component = np.abs(dy_acc(x_values))\n",
    "        y_component = np.abs(dy_acc(y_values))\n",
    "        z_component = np.abs(dy_acc(z_values))\n",
    "        \n",
    "        vectorial_sum = np.sqrt(x_component**2 + y_component**2 + z_component**2)\n",
    "        ave_vedba = np.nanmean(vectorial_sum)\n",
    "        \n",
    "        return pd.Series({\n",
    "            'tag': group_data['tag'].iloc[0],\n",
    "            'timestamp': group_data['timestamp'].iloc[0], \n",
    "            'ave_vedba': ave_vedba,\n",
    "            'n_samples': len(group_data)\n",
    "        })\n",
    "    \n",
    "    try:\n",
    "        filename = os.path.basename(file_path)\n",
    "        animal_id = filename.replace('.parquet', '')\n",
    "        \n",
    "        # Load the animal's accelerometer data\n",
    "        acc_data = pd.read_parquet(file_path)\n",
    "        \n",
    "        # Create grouping column\n",
    "        acc_data['group_id'] = acc_data['tag'] + '_' + acc_data['timestamp'].astype(str)\n",
    "        \n",
    "        # Process all groups for this animal\n",
    "        results = acc_data.groupby('group_id', group_keys=False).apply(process_group_simple_parallel).reset_index(drop=True)\n",
    "        \n",
    "        # Add log transformation\n",
    "        results['log_vedba'] = np.log(results['ave_vedba'])\n",
    "        \n",
    "        # Sort by timestamp\n",
    "        results = results.sort_values('timestamp').reset_index(drop=True)\n",
    "        \n",
    "        # Select final columns\n",
    "        vedba_data = results[['tag', 'timestamp', 'ave_vedba', 'log_vedba', 'n_samples']]\n",
    "        \n",
    "        # Save individual VEDBA file\n",
    "        output_file = f\"{animal_id}.parquet\"\n",
    "        output_path = os.path.join(vedba_dir, output_file)\n",
    "        vedba_data.to_parquet(output_path, index=False)\n",
    "        \n",
    "        return {\n",
    "            'animal_id': animal_id,\n",
    "            'input_file': filename,\n",
    "            'output_file': output_file,\n",
    "            'records': len(vedba_data),\n",
    "            'date_range': f\"{vedba_data['timestamp'].min()} to {vedba_data['timestamp'].max()}\",\n",
    "            'status': 'success'\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'animal_id': os.path.basename(file_path).replace('.parquet', ''),\n",
    "            'input_file': os.path.basename(file_path),\n",
    "            'error': str(e),\n",
    "            'status': 'error'\n",
    "        }\n",
    "\n",
    "# Get all input files\n",
    "data_folder = 'data/acc_v0'\n",
    "parquet_files = glob.glob(os.path.join(data_folder, '*.parquet'))\n",
    "\n",
    "# Determine optimal number of workers (use 75% of available cores)\n",
    "n_cores = mp.cpu_count()\n",
    "n_workers = max(1, int(n_cores * 0.75))  # Use 75% of cores, minimum 1\n",
    "\n",
    "print(f\"🚀 PARALLEL PROCESSING SETUP\")\n",
    "print(f\"📊 Found {len(parquet_files)} animal files to process\")\n",
    "print(f\"💻 Available CPU cores: {n_cores}\")\n",
    "print(f\"⚡ Using {n_workers} parallel workers\")\n",
    "print(f\"📁 Output directory: {vedba_dir}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"STARTING PARALLEL PROCESSING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "start_time = time.time()\n",
    "summary_list = []\n",
    "failed_files = []\n",
    "\n",
    "# Process files in parallel\n",
    "with ProcessPoolExecutor(max_workers=n_workers) as executor:\n",
    "    # Submit all jobs\n",
    "    future_to_file = {executor.submit(process_animal_file_parallel, file_path): file_path \n",
    "                     for file_path in parquet_files}\n",
    "    \n",
    "    # Collect results as they complete\n",
    "    for i, future in enumerate(as_completed(future_to_file), 1):\n",
    "        file_path = future_to_file[future]\n",
    "        result = future.result()\n",
    "        \n",
    "        if result['status'] == 'success':\n",
    "            summary_list.append(result)\n",
    "            print(f\"✅ [{i}/{len(parquet_files)}] {result['animal_id']}: {result['records']} records\")\n",
    "        else:\n",
    "            failed_files.append(result)\n",
    "            print(f\"❌ [{i}/{len(parquet_files)}] {result['animal_id']}: ERROR - {result['error']}\")\n",
    "\n",
    "end_time = time.time()\n",
    "processing_time = end_time - start_time\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"PARALLEL PROCESSING COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"⏱️  Total processing time: {processing_time:.2f} seconds ({processing_time/60:.1f} minutes)\")\n",
    "print(f\"✅ Successfully processed: {len(summary_list)} animals\")\n",
    "print(f\"❌ Failed: {len(failed_files)} animals\")\n",
    "print(f\"📊 Average time per animal: {processing_time/len(parquet_files):.2f} seconds\")\n",
    "\n",
    "if summary_list:\n",
    "    total_records = sum(s['records'] for s in summary_list)\n",
    "    print(f\"📈 Total VEDBA records created: {total_records:,}\")\n",
    "    print(f\"⚡ Processing speed: {total_records/processing_time:.0f} records/second\")\n",
    "\n",
    "# Show successful processing summary\n",
    "if summary_list:\n",
    "    print(f\"\\n📋 Successfully processed animals:\")\n",
    "    for summary in summary_list[:10]:  # Show first 10\n",
    "        print(f\"   {summary['animal_id']}: {summary['records']:,} records\")\n",
    "    if len(summary_list) > 10:\n",
    "        print(f\"   ... and {len(summary_list) - 10} more animals\")\n",
    "\n",
    "# Show any failures\n",
    "if failed_files:\n",
    "    print(f\"\\n⚠️  Failed processing:\")\n",
    "    for failed in failed_files:\n",
    "        print(f\"   {failed['animal_id']}: {failed['error']}\")\n",
    "\n",
    "# Verify output\n",
    "vedba_files = [f for f in os.listdir(vedba_dir) if f.endswith('.parquet')]\n",
    "print(f\"\\n📁 Output verification:\")\n",
    "print(f\"   {len(vedba_files)} VEDBA files created in {vedba_dir}/\")\n",
    "\n",
    "print(f\"\\n🎉 Parallel processing complete! Each animal's VEDBA data is in: {vedba_dir}/[AnimalID].parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
