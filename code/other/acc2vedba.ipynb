{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e0b3dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyreadr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from concurrent.futures import as_completed\n",
    "import ast\n",
    "\n",
    "import os\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b56157e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "from sklearn.utils import Bunch\n",
    "\n",
    "# Read in the complete data\n",
    "acc_data_trim = pd.read_parquet('data/acc_v0.parquet')  # Replace with the actual file path\n",
    "\n",
    "# Convert timestamp to local time\n",
    "#acc_data['timestamp'] = pd.to_datetime(acc_data['timestamp'], format='%Y-%m-%d %H:%M:%S', utc=True)\n",
    "#acc_data['local_timestamp'] = acc_data['timestamp'].dt.tz_convert('Africa/Nairobi')\n",
    "\n",
    "# Subset data\n",
    "#try:\n",
    "#    acc_data_trim = acc_data[['individual_local_identifier', 'local_timestamp', \n",
    "#                              'eobs_accelerations_raw', 'eobs_acceleration_sampling_frequency_per_axis']]\n",
    "#except KeyError as e:\n",
    "#    print(f\"KeyError: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71e0f6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Join with metadata\n",
    "metadata = pd.read_csv('data/metadata.csv')  # Replace with the actual file path\n",
    "acc_data_trim = acc_data_trim.merge(\n",
    "    metadata[['individual_local_identifier', 'tag_local_identifier', 'group_id']],\n",
    "    on='individual_local_identifier',\n",
    "    how='left'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b6d34a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/TOP/rharel/EAS_ind/rharel/analysis/JK_ch1/.venv/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Function to process a chunk of data\n",
    "def process_chunk(chunk):\n",
    "    return chunk['eobs_accelerations_raw'].str.split(' ', expand=True).astype(float)\n",
    "\n",
    "# Split the DataFrame into chunks\n",
    "num_chunks = 20  # Adjust based on the number of CPU cores\n",
    "chunks = np.array_split(acc_data_trim, num_chunks)\n",
    "\n",
    "# Process chunks in parallel\n",
    "with ProcessPoolExecutor(max_workers=num_chunks) as executor:\n",
    "    results = list(executor.map(process_chunk, chunks))\n",
    "\n",
    "# Combine the results back into a single DataFrame\n",
    "d2 = pd.concat(results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c03e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split eobs_accelerations_raw into separate columns\n",
    "d2 = acc_data_trim['eobs_accelerations_raw'].str.split(' ', expand=True).astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3554f1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Rename columns\n",
    "num_cols = d2.shape[1]\n",
    "col_names = [f\"{axis}{i+1}\" for i in range(num_cols // 3) for axis in ['x', 'y', 'z']]\n",
    "#d2.columns = col_names\n",
    "\n",
    "# Add timestamp and tag columns\n",
    "d2['local_timestamp'] = acc_data_trim['local_timestamp']\n",
    "d2['tag'] = acc_data_trim['tag_local_identifier_x']\n",
    "\n",
    "# Remove rows with missing values\n",
    "inds = d2.dropna().index\n",
    "acc_data_trim = acc_data_trim.loc[inds]\n",
    "d2 = d2.loc[inds]\n",
    "\n",
    "# Split into x, y, z components\n",
    "x_d = d2.filter(like='x')\n",
    "y_d = d2.filter(like='y')\n",
    "z_d = d2.filter(like='z')\n",
    "\n",
    "# Read calibration data\n",
    "acc_calib = pd.read_csv('data/acc_calib.csv')  # Replace with the actual file path\n",
    "acc_calib['tag'] = acc_calib['tag'].astype(str)\n",
    "acc_calib[['x0', 'y0', 'z0', 'Sx', 'Sy', 'Sz']] = acc_calib[['x0', 'y0', 'z0', 'Sx', 'Sy', 'Sz']].astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc258420",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Match indices between acc_calib and acc_data_trim\n",
    "ind_id = acc_data_trim['tag_local_identifier_x'].map(acc_calib.set_index('tag').index)\n",
    "\n",
    "# Calculate calibrated accelerations\n",
    "acc_data_trim['x_cal'] = (x_d.values - acc_calib.loc[ind_id, 'x0'].values[:, None]) * acc_calib.loc[ind_id, 'Sx'].values[:, None] * 9.81\n",
    "acc_data_trim['y_cal'] = (y_d.values - acc_calib.loc[ind_id, 'y0'].values[:, None]) * acc_calib.loc[ind_id, 'Sy'].values[:, None] * -9.81\n",
    "acc_data_trim['z_cal'] = (z_d.values - acc_calib.loc[ind_id, 'z0'].values[:, None]) * acc_calib.loc[ind_id, 'Sz'].values[:, None] * 9.81"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f4ccadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Helper function: dy_acc ----\n",
    "def dy_acc(vect, win_size=7):\n",
    "    \"\"\"\n",
    "    Calculate the dynamic acceleration (dy_acc) for a given vector.\n",
    "    \"\"\"\n",
    "    if vect is None or len(vect) == 0:\n",
    "        raise ValueError(\"Input vector is empty or invalid.\")\n",
    "    \n",
    "    pad_size = int(win_size / 2 - 0.5)\n",
    "    padded = np.pad(vect, (pad_size, pad_size), constant_values=np.nan)\n",
    "    acc_vec = np.empty(len(vect))\n",
    "    acc_vec[:] = np.nan\n",
    "\n",
    "    for i in range(len(vect)):\n",
    "        window = padded[i : i + (2 * pad_size + 1)]\n",
    "        m_ave = np.nanmean(window)\n",
    "        acc_vec[i] = vect[i] - m_ave\n",
    "    \n",
    "    return acc_vec\n",
    "\n",
    "def process_row(row):\n",
    "    \"\"\"\n",
    "    Process a single row to calculate dynamic acceleration components and derived metrics.\n",
    "    \"\"\"\n",
    "    x_component = np.abs(dy_acc(row['x_cal_array']))\n",
    "    y_component = np.abs(dy_acc(row['y_cal_array']))\n",
    "    z_component = np.abs(dy_acc(row['z_cal_array']))\n",
    "\n",
    "    vectorial_sum = np.sqrt(x_component**2 + y_component**2 + z_component**2)\n",
    "    ave_vedba_value = np.nanmean(vectorial_sum)\n",
    "\n",
    "    pitch = np.arctan2(x_component, np.sqrt(y_component**2 + z_component**2))\n",
    "    ave_pitch = np.nanmean(pitch)\n",
    "\n",
    "    return ave_vedba_value, ave_pitch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Main Processing ----\n",
    "# Load the data\n",
    "acc_data_trim = pd.read_parquet('data/acc_v1.parquet')\n",
    "acc_data_trim = acc_data_trim.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6eb99744",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m      7\u001b[39m acc_data_trim[\u001b[33m'\u001b[39m\u001b[33mz_cal_array\u001b[39m\u001b[33m'\u001b[39m] = acc_data_trim[\u001b[33m'\u001b[39m\u001b[33mz_cal\u001b[39m\u001b[33m'\u001b[39m].apply(\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m d: np.array(\u001b[38;5;28mlist\u001b[39m(d.values())) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(d, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m np.nan\n\u001b[32m      9\u001b[39m )\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Check for NaN or non-finite values in x_cal_array, y_cal_array, z_cal_array\u001b[39;00m\n\u001b[32m     13\u001b[39m invalid_rows = acc_data_trim[\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     \u001b[43macc_data_trim\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mx_cal_array\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mndarray\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43misfinite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m |\n\u001b[32m     15\u001b[39m     acc_data_trim[\u001b[33m'\u001b[39m\u001b[33my_cal_array\u001b[39m\u001b[33m'\u001b[39m].apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, np.ndarray) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np.isfinite(x).all()) |\n\u001b[32m     16\u001b[39m     acc_data_trim[\u001b[33m'\u001b[39m\u001b[33mz_cal_array\u001b[39m\u001b[33m'\u001b[39m].apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, np.ndarray) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np.isfinite(x).all())\n\u001b[32m     17\u001b[39m ]\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Print the invalid rows\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNumber of invalid rows: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(invalid_rows)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/EAS_ind/rharel/analysis/JK_ch1/.venv/lib/python3.12/site-packages/pandas/core/series.py:4924\u001b[39m, in \u001b[36mSeries.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[39m\n\u001b[32m   4789\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\n\u001b[32m   4790\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4791\u001b[39m     func: AggFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4796\u001b[39m     **kwargs,\n\u001b[32m   4797\u001b[39m ) -> DataFrame | Series:\n\u001b[32m   4798\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4799\u001b[39m \u001b[33;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[32m   4800\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4915\u001b[39m \u001b[33;03m    dtype: float64\u001b[39;00m\n\u001b[32m   4916\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   4917\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4918\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4919\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4920\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4921\u001b[39m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4922\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4923\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m4924\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/EAS_ind/rharel/analysis/JK_ch1/.venv/lib/python3.12/site-packages/pandas/core/apply.py:1427\u001b[39m, in \u001b[36mSeriesApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1424\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_compat()\n\u001b[32m   1426\u001b[39m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1427\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/EAS_ind/rharel/analysis/JK_ch1/.venv/lib/python3.12/site-packages/pandas/core/apply.py:1507\u001b[39m, in \u001b[36mSeriesApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1501\u001b[39m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[32m   1502\u001b[39m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[32m   1503\u001b[39m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[32m   1504\u001b[39m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[32m   1505\u001b[39m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[32m   1506\u001b[39m action = \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.dtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1507\u001b[39m mapped = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1508\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[32m   1509\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1511\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m   1512\u001b[39m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[32m   1513\u001b[39m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[32m   1514\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index=obj.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/EAS_ind/rharel/analysis/JK_ch1/.venv/lib/python3.12/site-packages/pandas/core/base.py:921\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    918\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    919\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m921\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/EAS_ind/rharel/analysis/JK_ch1/.venv/lib/python3.12/site-packages/pandas/core/algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mlib.pyx:2972\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36m<lambda>\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m      7\u001b[39m acc_data_trim[\u001b[33m'\u001b[39m\u001b[33mz_cal_array\u001b[39m\u001b[33m'\u001b[39m] = acc_data_trim[\u001b[33m'\u001b[39m\u001b[33mz_cal\u001b[39m\u001b[33m'\u001b[39m].apply(\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m d: np.array(\u001b[38;5;28mlist\u001b[39m(d.values())) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(d, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m np.nan\n\u001b[32m      9\u001b[39m )\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Check for NaN or non-finite values in x_cal_array, y_cal_array, z_cal_array\u001b[39;00m\n\u001b[32m     13\u001b[39m invalid_rows = acc_data_trim[\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     acc_data_trim[\u001b[33m'\u001b[39m\u001b[33mx_cal_array\u001b[39m\u001b[33m'\u001b[39m].apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, np.ndarray) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43misfinite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m.all()) |\n\u001b[32m     15\u001b[39m     acc_data_trim[\u001b[33m'\u001b[39m\u001b[33my_cal_array\u001b[39m\u001b[33m'\u001b[39m].apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, np.ndarray) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np.isfinite(x).all()) |\n\u001b[32m     16\u001b[39m     acc_data_trim[\u001b[33m'\u001b[39m\u001b[33mz_cal_array\u001b[39m\u001b[33m'\u001b[39m].apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, np.ndarray) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np.isfinite(x).all())\n\u001b[32m     17\u001b[39m ]\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Print the invalid rows\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNumber of invalid rows: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(invalid_rows)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''"
     ]
    }
   ],
   "source": [
    "acc_data_trim['x_cal_array'] = acc_data_trim['x_cal'].apply(\n",
    "    lambda d: np.array(list(d.values())) if isinstance(d, dict) else np.nan\n",
    ")\n",
    "acc_data_trim['y_cal_array'] = acc_data_trim['y_cal'].apply(\n",
    "    lambda d: np.array(list(d.values())) if isinstance(d, dict) else np.nan\n",
    ")\n",
    "acc_data_trim['z_cal_array'] = acc_data_trim['z_cal'].apply(\n",
    "    lambda d: np.array(list(d.values())) if isinstance(d, dict) else np.nan\n",
    ")\n",
    "\n",
    "\n",
    "# Check for NaN or non-finite values in x_cal_array, y_cal_array, z_cal_array\n",
    "invalid_rows = acc_data_trim[\n",
    "    acc_data_trim['x_cal_array'].apply(lambda x: not isinstance(x, np.ndarray) or not np.isfinite(x).all()) |\n",
    "    acc_data_trim['y_cal_array'].apply(lambda x: not isinstance(x, np.ndarray) or not np.isfinite(x).all()) |\n",
    "    acc_data_trim['z_cal_array'].apply(lambda x: not isinstance(x, np.ndarray) or not np.isfinite(x).all())\n",
    "]\n",
    "\n",
    "# Print the invalid rows\n",
    "print(f\"Number of invalid rows: {len(invalid_rows)}\")\n",
    "print(invalid_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157b11fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with joblib\n",
    "# # from joblib import Parallel, delayed\n",
    "\n",
    "# worker_count = 8\n",
    "# results = Parallel(n_jobs=worker_count)(delayed(process_row)(row) for _, row in acc_data_trim[:200000].iterrows())\n",
    "\n",
    "# with chunks\n",
    "# results = []\n",
    "# chunk_size = 100000  # Adjust the chunk size as needed\n",
    "# data_chunks = list(chunk_data(acc_data_trim.iterrows(), chunk_size))\n",
    "\n",
    "# with ProcessPoolExecutor(max_workers=16) as executor:\n",
    "#     futures = list(tqdm(executor.map(process_chunk, data_chunks), total=len(data_chunks), desc=\"Collecting results\"))\n",
    "#     for future in futures:\n",
    "#         results.extend(future)\n",
    "\n",
    "results = []\n",
    "with ProcessPoolExecutor(max_workers=8) as executor:\n",
    "    futures = {executor.submit(process_row, row): i for i, row in acc_data_trim.iterrows()}\n",
    "    for f in futures:\n",
    "        results.append(f.result())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36305a61",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'process_row' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31m_RemoteTraceback\u001b[39m                          Traceback (most recent call last)",
      "\u001b[31m_RemoteTraceback\u001b[39m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/usr/lib/python3.12/concurrent/futures/process.py\", line 263, in _process_worker\n    r = call_item.fn(*call_item.args, **call_item.kwargs)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_3885519/2555356842.py\", line 20, in process_chunk\n    results = [process_row(row) for _, row in chunk.iterrows()]\n               ^^^^^^^^^^^\nNameError: name 'process_row' is not defined\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 45\u001b[39m\n\u001b[32m     43\u001b[39m     \u001b[38;5;66;03m# Collect results as they complete\u001b[39;00m\n\u001b[32m     44\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m futures:\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m         results.append(\u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# Combine all processed chunks into a single DataFrame\u001b[39;00m\n\u001b[32m     48\u001b[39m acc_data_trim = pd.concat(results, ignore_index=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'process_row' is not defined"
     ]
    }
   ],
   "source": [
    "######################################\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "# Function to process a chunk of data\n",
    "def process_chunk(chunk):\n",
    "    # Convert 'x_cal', 'y_cal', 'z_cal' dict columns to numpy arrays\n",
    "    chunk['x_cal_array'] = chunk['x_cal'].apply(\n",
    "        lambda d: np.array(list(d.values())) if isinstance(d, dict) else np.nan\n",
    "    )\n",
    "    chunk['y_cal_array'] = chunk['y_cal'].apply(\n",
    "        lambda d: np.array(list(d.values())) if isinstance(d, dict) else np.nan\n",
    "    )\n",
    "    chunk['z_cal_array'] = chunk['z_cal'].apply(\n",
    "        lambda d: np.array(list(d.values())) if isinstance(d, dict) else np.nan\n",
    "    )\n",
    "    # Process each row in the chunk\n",
    "    results = [process_row(row) for _, row in chunk.iterrows()]\n",
    "    ave_vedba, ave_pitch = zip(*results)\n",
    "    chunk['ave_vedba'] = ave_vedba\n",
    "    chunk['ave_pitch'] = ave_pitch\n",
    "    return chunk\n",
    "\n",
    "# Read the Parquet file in chunks\n",
    "file_path = 'data/acc_v1.parquet'  # Replace with your file path\n",
    "parquet_file = pq.ParquetFile(file_path)\n",
    "\n",
    "# Process the file in chunks\n",
    "chunk_size = 10000  # Define the number of rows per chunk\n",
    "results = []\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=40) as executor:\n",
    "    futures = []\n",
    "    for i in range(0, parquet_file.num_row_groups):\n",
    "        # Read a chunk (row group)\n",
    "        table = parquet_file.read_row_group(i)\n",
    "        chunk = table.to_pandas()\n",
    "        # Submit the chunk for parallel processing\n",
    "        futures.append(executor.submit(process_chunk, chunk))\n",
    "    \n",
    "    # Collect results as they complete\n",
    "    for future in futures:\n",
    "        results.append(future.result())\n",
    "\n",
    "# Combine all processed chunks into a single DataFrame\n",
    "acc_data_trim = pd.concat(results, ignore_index=True)\n",
    "\n",
    "# Save the processed data\n",
    "#acc_data_trim.to_parquet('data/processed_acc_v1.parquet', index=False)\n",
    "#print(\"Data saved to 'data/processed_acc_v1.parquet'\")\n",
    "######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bea819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to 'data/acc_vedba.parquet'\n"
     ]
    }
   ],
   "source": [
    "# Store back the results\n",
    "ave_vedba, ave_pitch = zip(*results)\n",
    "acc_data_trim['ave_vedba'] = ave_vedba\n",
    "acc_data_trim['ave_pitch'] = ave_pitch\n",
    "\n",
    "# Log-transform like in R\n",
    "acc_data_trim['log_vedba'] = np.log(acc_data_trim['ave_vedba'])\n",
    "\n",
    "# Select relevant columns\n",
    "filtered_data = acc_data_trim[['individual_local_identifier', \n",
    "                               'local_timestamp', \n",
    "                               'tag_local_identifier', \n",
    "                               'log_vedba', \n",
    "                               'ave_pitch']]\n",
    "\n",
    "# Save the processed data\n",
    "filtered_data.to_parquet('data/acc_vedba.parquet', index=False)\n",
    "print(\"Data saved to 'data/acc_vedba.parquet'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
